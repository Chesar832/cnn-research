{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m KAGGLE_INPUT_PATH\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/kaggle/input\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m KAGGLE_INPUT_SYMLINK\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKAGGLE_INPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m777\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m os\u001b[38;5;241m.\u001b[39msymlink(KAGGLE_INPUT_PATH, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m), target_is_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(KAGGLE_INPUT_SYMLINK)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/kaggle'"
     ]
    }
   ],
   "source": [
    "# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATASETS\n",
    "# # TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
    "# # THEN FEEL FREE TO DELETE CELL.\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# from tempfile import NamedTemporaryFile\n",
    "# from urllib.request import urlopen\n",
    "# from urllib.parse import unquote\n",
    "# from urllib.error import HTTPError\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "# CHUNK_SIZE = 40960\n",
    "# DATASET_MAPPING = ':https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F729820%2F1267148%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20230902%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20230902T203321Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D89a2e63a48c344a6b7acab1d8b170cef0eeb843c2fe13eda5e469b4daacb98e0fc60f7447a3415c6702e8b16f5545903e682cc89ac17b0b5c1b9e89f60e69f8beda15e9ce9bb726487e86c657fb19de889253469653fa7a16c1a03cac3b7d0ae3df9e57d6b1577078826420ba1212ec74050921e8aa86be61c0037b7950b164a02ae79837542dfcd9451eb49c13864337c4789f25baacec48c517287de343d2eda6a6e4d6b91dd515ebfac95ba25b16ae1658cc1f784a22bfce7a26ec7423bbcb701b31660278386223440fdc7dd480840609f0b1898657663aedc365179b9d0ecb65df9618f19a8d8303c0417c91d065cdbda8e5dece2c31ab8def35d52e047'\n",
    "# KAGGLE_INPUT_PATH='/home/kaggle/input'\n",
    "# KAGGLE_INPUT_SYMLINK='/kaggle'\n",
    "\n",
    "# os.makedirs(KAGGLE_INPUT_PATH, 777)\n",
    "# os.symlink(KAGGLE_INPUT_PATH, os.path.join('..', 'input'), target_is_directory=True)\n",
    "# os.makedirs(KAGGLE_INPUT_SYMLINK)\n",
    "# os.symlink(KAGGLE_INPUT_PATH, os.path.join(KAGGLE_INPUT_SYMLINK, 'input'), target_is_directory=True)\n",
    "\n",
    "# for dataset_mapping in DATASET_MAPPING.split(','):\n",
    "#     directory, download_url_encoded = dataset_mapping.split(':')\n",
    "#     download_url = unquote(download_url_encoded)\n",
    "#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "#     try:\n",
    "#         with urlopen(download_url) as zipfileres, NamedTemporaryFile() as tfile:\n",
    "#             total_length = zipfileres.headers['content-length']\n",
    "#             print(f'Downloading {directory}, {total_length} bytes zipped')\n",
    "#             dl = 0\n",
    "#             data = zipfileres.read(CHUNK_SIZE)\n",
    "#             while len(data) > 0:\n",
    "#                 dl += len(data)\n",
    "#                 tfile.write(data)\n",
    "#                 done = int(50 * dl / int(total_length))\n",
    "#                 sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "#                 sys.stdout.flush()\n",
    "#                 data = zipfileres.read(CHUNK_SIZE)\n",
    "#             print(f'\\nUnzipping {directory}')\n",
    "#             with ZipFile(tfile) as zfile:\n",
    "#                 zfile.extractall(destination_path)\n",
    "#     except HTTPError as e:\n",
    "#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "#     except OSError as e:\n",
    "#         print(f'Failed to load {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "# print('Dataset import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "from matplotlib.patches import Rectangle\n",
    "from lxml import etree\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Image_Label_Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../input/images/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../input/label/label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = glob.glob('../input/images/images/*/*.jpg')\n",
    "len(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_path = glob.glob('../input/label/label/*.xml')\n",
    "len(xmls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_path[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml_name extraction\n",
    "xmls_train = [p.split('/')[-1].split('.')[0] for p in xmls_path]\n",
    "xmls_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_name extraction\n",
    "imgs_train = [img for img in image_path if (img.split('/')[-1].split)('.jpg')[0] in xmls_train]\n",
    "imgs_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs_train),len(xmls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the image to label sorts\n",
    "xmls_path.sort(key=lambda x:x.split('/')[-1].split('.xml')[0])\n",
    "imgs_train.sort(key=lambda x:x.split('/')[-1].split('.jpg')[0])\n",
    "xmls_path[:3],imgs_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels names\n",
    "names = [x.split(\"/\")[-2] for x in imgs_train]\n",
    "names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.DataFrame(names,columns=['Types'])\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot for mutiple classes\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "Class = names['Types'].unique()\n",
    "Class_dict = dict(zip(Class, range(1,len(Class)+1)))\n",
    "names['str'] = names['Types'].apply(lambda x: Class_dict[x])\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(list(Class_dict.values()))\n",
    "transformed_labels = lb.transform(names['str'])\n",
    "y_bin_labels = []  \n",
    "\n",
    "for i in range(transformed_labels.shape[1]):\n",
    "    y_bin_labels.append('str' + str(i))\n",
    "    names['str' + str(i)] = transformed_labels[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.drop('str',axis=1,inplace=True)\n",
    "names.drop('Types',axis=1,inplace=True)\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Extraction & Input pipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis rectangular box value in xmls\n",
    "def to_labels(path):\n",
    "    xml = open('{}'.format(path)).read()                         #read xml in path \n",
    "    sel = etree.HTML(xml)                     \n",
    "    width = int(sel.xpath('//size/width/text()')[0])     #extract the width/height\n",
    "    height = int(sel.xpath('//size/height/text()')[0])    #extract the x,y value\n",
    "    xmin = int(sel.xpath('//bndbox/xmin/text()')[0])\n",
    "    xmax = int(sel.xpath('//bndbox/xmax/text()')[0])\n",
    "    ymin = int(sel.xpath('//bndbox/ymin/text()')[0])\n",
    "    ymax = int(sel.xpath('//bndbox/ymax/text()')[0])\n",
    "    return [xmin/width, ymin/height, xmax/width, ymax/height]   #return the four relative points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set value to labels\n",
    "labels = [to_labels(path) for path in xmls_path]\n",
    "labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set four labels as outputs\n",
    "out1,out2,out3,out4 = list(zip(*labels))        \n",
    "#convert to np.array\n",
    "out1 = np.array(out1)\n",
    "out2 = np.array(out2)\n",
    "out3 = np.array(out3)\n",
    "out4 = np.array(out4)\n",
    "label = np.array(names.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label to tf.data\n",
    "label_datasets = tf.data.Dataset.from_tensor_slices((out1,out2,out3,out4,label))\n",
    "label_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_image function\n",
    "def load_image(path):\n",
    "    image = tf.io.read_file(path)                           \n",
    "    image = tf.image.decode_jpeg(image,3)               \n",
    "    image = tf.image.resize(image,[224,224])               \n",
    "    image = tf.cast(image/127.5-1,tf.float32)                 \n",
    "    return image      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(imgs_train)\n",
    "dataset = dataset.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = tf.data.Dataset.zip((dataset,label_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch constant\n",
    "BATCH_SIZE = 16\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch extraction and shuffle\n",
    "dataset_label = dataset_label.repeat().shuffle(500).batch(BATCH_SIZE)\n",
    "dataset_label = dataset_label.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "test_count = int(len(imgs_train)*0.2)\n",
    "train_count = len(imgs_train) - test_count\n",
    "test_count,train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_label.skip(test_count)\n",
    "test_dataset = dataset_label.take(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_dict = {v:k for k,v in Class_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check from train_data\n",
    "for img, label in train_dataset.take(1):\n",
    "    plt.imshow(keras.preprocessing.image.array_to_img(img[0]))     \n",
    "    out1,out2,out3,out4,out5 = label                            \n",
    "    xmin,ymin,xmax,ymax = out1[0].numpy()*224,out2[0].numpy()*224,out3[0].numpy()*224,out4[0].numpy()*224\n",
    "    rect = Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),fill=False,color='r')  \n",
    "    ax = plt.gca()                      \n",
    "    ax.axes.add_patch(rect)   \n",
    "    pred_imglist = []\n",
    "    pred_imglist.append(species_dict[np.argmax(out5[0])+1])\n",
    "    plt.title(pred_imglist)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 The VGG16 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution based\n",
    "conv = keras.applications.xception.Xception(weights='imagenet',\n",
    "                                            include_top=False,\n",
    "                                            input_shape=(224,224,3),\n",
    "                                            pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open trainable\n",
    "conv.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Conv + FC structure\n",
    "inputs = keras.Input(shape=(224,224,3))\n",
    "x = conv(inputs)\n",
    "x1 = keras.layers.Dense(1024,activation='relu')(x)\n",
    "x1 = keras.layers.Dense(512,activation='relu')(x1)\n",
    "\n",
    "\n",
    "out1 = keras.layers.Dense(1,name='out1')(x1)\n",
    "out2 = keras.layers.Dense(1,name='out2')(x1)\n",
    "out3 = keras.layers.Dense(1,name='out3')(x1)\n",
    "out4 = keras.layers.Dense(1,name='out4')(x1)\n",
    "\n",
    "x2 = keras.layers.Dense(1024,activation='relu')(x)\n",
    "x2 = keras.layers.Dropout(0.5)(x2)\n",
    "x2 = keras.layers.Dense(512,activation='relu')(x2)\n",
    "out_class = keras.layers.Dense(10,activation='softmax',name='out_item')(x2)\n",
    "\n",
    "out = [out1,out2,out3,out4,out_class]\n",
    "\n",
    "model = keras.models.Model(inputs=inputs,outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Compile & Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model compille\n",
    "model.compile(keras.optimizers.Adam(0.0003),\n",
    "              loss={'out1':'mse',\n",
    "                    'out2':'mse',\n",
    "                    'out3':'mse',\n",
    "                    'out4':'mse',\n",
    "                    'out_item':'categorical_crossentropy'},\n",
    "              metrics=['mae','acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate reduce module\n",
    "lr_reduce = keras.callbacks.ReduceLROnPlateau('val_loss', patience=6, factor=0.5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                   steps_per_epoch=train_count//BATCH_SIZE,\n",
    "                   epochs=200,\n",
    "                   validation_data=test_dataset,\n",
    "                   validation_steps=test_count//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training visualization\n",
    "def plot_history(history):                \n",
    "    hist = pd.DataFrame(history.history)           \n",
    "    hist['epoch']=history.epoch\n",
    "    \n",
    "    plt.figure()                                     \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')               \n",
    "    plt.plot(hist['epoch'],hist['loss'],\n",
    "            label='Train Loss')\n",
    "    plt.plot(hist['epoch'],hist['val_loss'],\n",
    "            label='Val Loss')                           \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()                                      \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Val_MAE')               \n",
    "    plt.plot(hist['epoch'],hist['val_out1_mae'],\n",
    "            label='Out1_mae')\n",
    "    plt.plot(hist['epoch'],hist['val_out2_mae'],\n",
    "            label='Out2_mae')\n",
    "    plt.plot(hist['epoch'],hist['val_out3_mae'],\n",
    "            label='Out3_mae')\n",
    "    plt.plot(hist['epoch'],hist['val_out4_mae'],\n",
    "            label='Out4_mae')\n",
    "    plt.legend()      \n",
    "    \n",
    "    plt.figure()                                      \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Val_Item_Acc')               \n",
    "    plt.plot(hist['epoch'],hist['val_out_item_acc'],\n",
    "            label='Out5_acc')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('out1_mae in test:{}'.format(mae[6]))\n",
    "print('out2_mae in test:{}'.format(mae[8]))\n",
    "print('out3_mae in test:{}'.format(mae[10]))\n",
    "print('out4_mae in test:{}'.format(mae[12]))\n",
    "print('class_label in test:{}'.format(mae[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"class_location.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_dict = {v:k for k,v in Class_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,24))\n",
    "for img,_ in train_dataset.take(1):\n",
    "    out1,out2,out3,out4,label = model.predict(img)\n",
    "    for i in range(3):\n",
    "        plt.subplot(3,1,i+1)            \n",
    "        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n",
    "        pred_imglist = []\n",
    "        pred_imglist.append(species_dict[np.argmax(out5[i])+1])\n",
    "        plt.title(pred_imglist)\n",
    "        xmin,ymin,xmax,ymax = out1[i]*224,out2[i]*224,out3[i]*224,out4[i]*224\n",
    "        rect = Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),fill=False,color='r') \n",
    "        ax = plt.gca()                   \n",
    "        ax.axes.add_patch(rect)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
